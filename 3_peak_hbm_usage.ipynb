{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb34398",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "import pyarrow.compute as pc\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "from setup_plot import setup_local, get_colors, get_markers\n",
    "\n",
    "\n",
    "SET1_PARQUET     = \"ldms.parquet\"\n",
    "SACCT_CSV        = Path(\"sacct.csv\")\n",
    "NODES_80GB_FILE  = Path(\"nodes_80gb.txt\")\n",
    "\n",
    "CAP_40_MIB = 40960\n",
    "CAP_80_MIB = 81920\n",
    "BATCH_SIZE = 1_000_000\n",
    "\n",
    "\n",
    "def load_nodes_from_file(path: Path) -> set[str]:\n",
    "    \"\"\"\n",
    "    Reads hostnames from a text file (one hostname per line).\n",
    "    \"\"\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"80GB node list not found: {path.resolve()}\")\n",
    "    nodes: set[str] = set()\n",
    "    for ln in path.read_text(encoding=\"utf-8\", errors=\"replace\").splitlines():\n",
    "        ln = ln.strip()\n",
    "        if not ln or ln.startswith(\"#\"):\n",
    "            continue\n",
    "        for tok in ln.replace(\",\", \" \").split():\n",
    "            tok = tok.strip()\n",
    "            if tok and not tok.startswith(\"#\"):\n",
    "                nodes.add(tok)\n",
    "    return nodes\n",
    "\n",
    "\n",
    "def read_pipe_csv(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads a pipe-delimited sacct CSV.\n",
    "    \"\"\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"sacct file not found: {path.resolve()}\")\n",
    "\n",
    "    with path.open(\"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        header = f.readline().rstrip(\"\\n\")\n",
    "    names = header.split(\"|\")\n",
    "    ncols = len(names)\n",
    "\n",
    "    rows = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        next(f)\n",
    "        for ln in f:\n",
    "            parts = ln.rstrip(\"\\n\").split(\"|\", ncols - 1)\n",
    "            if len(parts) < ncols:\n",
    "                parts += [\"\"] * (ncols - len(parts))\n",
    "            rows.append(parts)\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=names)\n",
    "    if \"JobID\" not in df.columns:\n",
    "        raise KeyError(\"sacct CSV missing required column: JobID\")\n",
    "    df[\"JobID\"] = df[\"JobID\"].astype(str)\n",
    "    return df\n",
    "\n",
    "\n",
    "def requested_hbm_from_constraints(s: str):\n",
    "    s = (s or \"\").lower()\n",
    "    toks = [t.strip() for t in re.split(r\"[&,+]\", s) if t.strip()]\n",
    "    if \"hbm80g\" in toks:\n",
    "        return \"requested_80gb\"\n",
    "    if \"hbm40g\" in toks:\n",
    "        return \"requested_40gb\"\n",
    "    return \"unspecified\"\n",
    "\n",
    "\n",
    "def requested_hbm_from_submitline(s: str):\n",
    "    s = (s or \"\")\n",
    "    m = re.search(r\"(?:-C|--constraint)\\s*=?\\s*([^\\s]+)\", s)\n",
    "    return requested_hbm_from_constraints(m.group(1)) if m else \"unspecified\"\n",
    "\n",
    "\n",
    "nodes_80gb_set = load_nodes_from_file(NODES_80GB_FILE)\n",
    "nodes80_arr = pa.array(sorted(nodes_80gb_set), type=pa.string())\n",
    "\n",
    "_OCC_TABLE = None\n",
    "\n",
    "\n",
    "def build_job_occ_parquet():\n",
    "    global _OCC_TABLE\n",
    "\n",
    "    job_cap_max = defaultdict(int)\n",
    "    job_occ_max = defaultdict(float)\n",
    "\n",
    "    dset = ds.dataset(SET1_PARQUET, format=\"parquet\")\n",
    "    scanner = dset.scanner(\n",
    "        columns=[\"JobID\", \"hostname\", \"nersc_ldms_dcgm_fb_used\"],\n",
    "        batch_size=BATCH_SIZE,\n",
    "        use_threads=True,\n",
    "    )\n",
    "\n",
    "    nb = 0\n",
    "    for rb in scanner.to_batches():\n",
    "        nb += 1\n",
    "\n",
    "        job  = rb[\"JobID\"]\n",
    "        host = rb[\"hostname\"]\n",
    "        used = rb[\"nersc_ldms_dcgm_fb_used\"]\n",
    "\n",
    "        job_s  = pc.cast(job, pa.string())\n",
    "        used_f = pc.cast(used, pa.float64())\n",
    "        used_f = pc.fill_null(used_f, pa.scalar(0.0))\n",
    "\n",
    "        is80 = pc.is_in(host, value_set=nodes80_arr)\n",
    "        cap_i64 = pc.if_else(\n",
    "            is80,\n",
    "            pa.scalar(CAP_80_MIB, pa.int64()),\n",
    "            pa.scalar(CAP_40_MIB, pa.int64()),\n",
    "        )\n",
    "\n",
    "        occ = pc.divide(used_f, pc.cast(cap_i64, pa.float64()))\n",
    "\n",
    "        t = pa.table({\"JobID\": job_s, \"cap_nom\": cap_i64, \"occ_nom\": occ})\n",
    "        gb = t.group_by(\"JobID\").aggregate([(\"cap_nom\", \"max\"), (\"occ_nom\", \"max\")])\n",
    "\n",
    "        jarr   = gb[\"JobID\"].to_pylist()\n",
    "        caparr = gb[\"cap_nom_max\"].to_pylist()\n",
    "        occarr = gb[\"occ_nom_max\"].to_pylist()\n",
    "\n",
    "        for J, C, O in zip(jarr, caparr, occarr):\n",
    "            if J is None:\n",
    "                continue\n",
    "            J = str(J)\n",
    "\n",
    "            c = int(C) if C is not None else 0\n",
    "            o = float(O) if O is not None else float(\"nan\")\n",
    "\n",
    "            if c > job_cap_max[J]:\n",
    "                job_cap_max[J] = c\n",
    "            if o > job_occ_max[J]:\n",
    "                job_occ_max[J] = o\n",
    "\n",
    "    jobids = sorted(job_cap_max.keys())\n",
    "    out_tbl = pa.table({\n",
    "        \"JobID\": pa.array(jobids, type=pa.string()),\n",
    "        \"capacity_nom_mib\": pa.array([job_cap_max[j] for j in jobids], type=pa.int64()),\n",
    "        \"max_occ_nom\": pa.array([job_occ_max.get(j, float(\"nan\")) for j in jobids], type=pa.float64()),\n",
    "    })\n",
    "\n",
    "    _OCC_TABLE = out_tbl\n",
    "\n",
    "\n",
    "def plot_fb_used_80_req80():\n",
    "    T = _OCC_TABLE.combine_chunks()\n",
    "    occ = T.to_pandas()\n",
    "    occ[\"JobID\"] = occ[\"JobID\"].astype(str)\n",
    "\n",
    "    job_cap     = occ.groupby(\"JobID\", as_index=True)[\"capacity_nom_mib\"].max()\n",
    "    job_max_occ = occ.groupby(\"JobID\", as_index=True)[\"max_occ_nom\"].max()\n",
    "\n",
    "    sacct_df = read_pipe_csv(SACCT_CSV)\n",
    "\n",
    "    if \"Constraints\" in sacct_df.columns:\n",
    "        sacct_df[\"requested_gpu_mem\"] = sacct_df[\"Constraints\"].apply(requested_hbm_from_constraints)\n",
    "    else:\n",
    "        sacct_df[\"requested_gpu_mem\"] = \"unspecified\"\n",
    "\n",
    "    if \"SubmitLine\" in sacct_df.columns:\n",
    "        mask_unspec = sacct_df[\"requested_gpu_mem\"].eq(\"unspecified\")\n",
    "        sacct_df.loc[mask_unspec, \"requested_gpu_mem\"] = sacct_df.loc[mask_unspec, \"SubmitLine\"].apply(requested_hbm_from_submitline)\n",
    "\n",
    "    req80_ids = set(\n",
    "        sacct_df.loc[sacct_df[\"requested_gpu_mem\"].eq(\"requested_80gb\"), \"JobID\"].astype(str)\n",
    "    )\n",
    "\n",
    "    mask_80placed = job_cap.eq(CAP_80_MIB)\n",
    "    keep_ids = job_cap.index[mask_80placed & job_cap.index.isin(req80_ids)]\n",
    "    vals = job_max_occ.loc[keep_ids].values\n",
    "\n",
    "    vals_pct = np.clip(vals * 100.0, 0, 100)\n",
    "\n",
    "    edges = np.linspace(0, 100, 11)\n",
    "    counts, edges = np.histogram(vals_pct, bins=edges)\n",
    "\n",
    "    cdf_vals = (np.cumsum(counts) / counts.sum() * 100.0) if counts.sum() > 0 else np.zeros_like(counts, dtype=float)\n",
    "    bin_left  = edges[:-1]\n",
    "    bin_width = np.diff(edges)\n",
    "    bin_cent  = bin_left + bin_width / 2.0\n",
    "\n",
    "    setup_local()\n",
    "    colors  = get_colors()\n",
    "    markers = get_markers()\n",
    "\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    ax1.bar(bin_left, counts, width=bin_width, align=\"edge\",\n",
    "            color=colors[2], edgecolor=\"black\", label=\"Number of jobs\")\n",
    "\n",
    "    ax1.set_xlabel(\"Peak HBM usage (% of capacity)\", fontsize=17)\n",
    "    ax1.set_ylabel(\"Number of jobs\", fontsize=17)\n",
    "    ax1.set_xlim(0, 100)\n",
    "    ax1.set_xticks(np.arange(0, 101, 10))\n",
    "    ax1.tick_params(axis=\"x\", labelsize=16)\n",
    "    ax1.tick_params(axis=\"y\", labelsize=16)\n",
    "    ax1.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "    ax1.set_title(\"Distribution of jobs by HBM_USED (80 GB)\", fontsize=18, pad=18)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(bin_cent, cdf_vals, color=colors[0], marker=markers[2],\n",
    "             label=\"CDF (number of jobs)\", linewidth=2, clip_on=False)\n",
    "    ax2.set_ylabel(\"Cumulative percentage (%)\", fontsize=17)\n",
    "    ax2.set_ylim(0, 100)\n",
    "    ax2.set_yticks([0, 20, 40, 60, 80, 100])\n",
    "    ax2.tick_params(axis=\"y\", labelsize=16)\n",
    "\n",
    "    h1, l1 = ax1.get_legend_handles_labels()\n",
    "    h2, l2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(h1 + h2, l1 + l2, loc=\"best\", fontsize=16, framealpha=0.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "build_job_occ_parquet()\n",
    "plot_fb_used_80_req80()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
