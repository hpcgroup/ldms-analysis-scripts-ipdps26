{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088a522e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import duckdb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import pyarrow.parquet as pq\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "from setup_plot import setup_local\n",
    "\n",
    "\n",
    "SET1 = \"ldms_set1.parquet\"\n",
    "NODES_80GB_FILE = \"nodes_80gb.txt\"\n",
    "\n",
    "OUT_MEANS  = \"job_means_all.parquet\"\n",
    "OUT_METRICS = \"job_metrics.parquet\"\n",
    "OUT_LABELS = \"job_label_fractions_fp64only.parquet\"\n",
    "\n",
    "UTIL_COL = \"nersc_ldms_dcgm_gpu_utilization\"\n",
    "\n",
    "def build_job_means_all():\n",
    "    COUNTERS = [\n",
    "        \"nersc_ldms_dcgm_fp64_active\",\n",
    "        \"nersc_ldms_dcgm_dram_active\",\n",
    "    ]\n",
    "    def avg_expr(cols):\n",
    "        return \",\\n       \".join([f\"avg({c}) AS {c}\" for c in cols])\n",
    "\n",
    "    con = duckdb.connect()\n",
    "    con.execute(\"PRAGMA memory_limit='15GB';\")\n",
    "    con.execute(f\"PRAGMA threads={os.cpu_count() or 1};\")\n",
    "\n",
    "    con.execute(f\"\"\"\n",
    "    COPY (\n",
    "      SELECT\n",
    "        JobID,\n",
    "        {avg_expr(COUNTERS)}\n",
    "      FROM (\n",
    "        SELECT\n",
    "          JobID::VARCHAR AS JobID,\n",
    "          hostname,\n",
    "          gpu_id,\n",
    "          {avg_expr(COUNTERS)}\n",
    "        FROM parquet_scan('{SET1}')\n",
    "        GROUP BY JobID, hostname, gpu_id\n",
    "      )\n",
    "      GROUP BY JobID\n",
    "    )\n",
    "    TO '{OUT_MEANS}' (FORMAT PARQUET, COMPRESSION 'SNAPPY');\n",
    "    \"\"\")\n",
    "\n",
    "    con.close()\n",
    "\n",
    "def build_job_metrics_mean_util():\n",
    "    con = duckdb.connect()\n",
    "    con.execute(\"PRAGMA memory_limit='15GB';\")\n",
    "    con.execute(f\"PRAGMA threads={os.cpu_count() or 1};\")\n",
    "\n",
    "    con.execute(f\"\"\"\n",
    "    COPY (\n",
    "      SELECT\n",
    "        JobID,\n",
    "        AVG(gpu_mean) AS mean_utilization\n",
    "      FROM (\n",
    "        SELECT\n",
    "          JobID::VARCHAR AS JobID,\n",
    "          hostname,\n",
    "          gpu_id,\n",
    "          AVG({UTIL_COL}) AS gpu_mean\n",
    "        FROM parquet_scan('{SET1}')\n",
    "        GROUP BY JobID, hostname, gpu_id\n",
    "      )\n",
    "      GROUP BY JobID\n",
    "    )\n",
    "    TO '{OUT_METRICS}' (FORMAT PARQUET, COMPRESSION 'SNAPPY');\n",
    "    \"\"\")\n",
    "\n",
    "    con.close()\n",
    "\n",
    "def load_nodes_80gb(path: str) -> set[str]:\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Missing {p.resolve()}\")\n",
    "    nodes = set()\n",
    "    for ln in p.read_text(encoding=\"utf-8\", errors=\"replace\").splitlines():\n",
    "        ln = ln.strip()\n",
    "        if not ln or ln.startswith(\"#\"):\n",
    "            continue\n",
    "        for tok in ln.replace(\",\", \" \").split():\n",
    "            tok = tok.strip()\n",
    "            if tok and not tok.startswith(\"#\"):\n",
    "                nodes.add(tok)\n",
    "    return nodes\n",
    "\n",
    "def build_fp64only_label_fractions():\n",
    "    DCGM_PREFIX = \"nersc_ldms_dcgm_\"\n",
    "    ACTIVE_COLS = [\n",
    "        f\"{DCGM_PREFIX}fp16_active\",\n",
    "        f\"{DCGM_PREFIX}fp32_active\",\n",
    "        f\"{DCGM_PREFIX}fp64_active\",\n",
    "        f\"{DCGM_PREFIX}tensor_active\",\n",
    "        f\"{DCGM_PREFIX}dram_active\",\n",
    "    ]\n",
    "\n",
    "    PEAK_FLOPS_FP64_VECTOR = 9.7e12\n",
    "    HBM_40 = 1.555e12\n",
    "    HBM_80 = 2.039e12\n",
    "    BATCH = 1_000_000\n",
    "\n",
    "    nodes80 = load_nodes_80gb(NODES_80GB_FILE)\n",
    "\n",
    "    need_cols = [\"JobID\", \"hostname\", \"gpu_id\", \"ts_ns\"] + ACTIVE_COLS\n",
    "    pf = pq.ParquetFile(SET1)\n",
    "    present = set(pf.schema_arrow.names)\n",
    "    missing = [c for c in need_cols if c not in present]\n",
    "    if missing:\n",
    "        raise KeyError(f\"Missing required columns in {SET1}: {missing}\")\n",
    "\n",
    "    state = {} \n",
    "    job_time = defaultdict(lambda: {\"Compute-intensive\": 0, \"Memory-intensive\": 0, \"Other\": 0})\n",
    "    job_total = defaultdict(int)\n",
    "    job_samples = defaultdict(int)\n",
    "\n",
    "    for batch in pf.iter_batches(columns=need_cols, batch_size=BATCH):\n",
    "        df = batch.to_pandas()\n",
    "        if df.empty:\n",
    "            continue\n",
    "\n",
    "        job  = df[\"JobID\"].astype(str)\n",
    "        host = df[\"hostname\"].astype(str)\n",
    "        gpu  = pd.to_numeric(df[\"gpu_id\"], errors=\"coerce\")\n",
    "        ts   = pd.to_numeric(df[\"ts_ns\"], errors=\"coerce\")\n",
    "\n",
    "        fp16 = pd.to_numeric(df[f\"{DCGM_PREFIX}fp16_active\"], errors=\"coerce\")\n",
    "        fp32 = pd.to_numeric(df[f\"{DCGM_PREFIX}fp32_active\"], errors=\"coerce\")\n",
    "        fp64 = pd.to_numeric(df[f\"{DCGM_PREFIX}fp64_active\"], errors=\"coerce\")\n",
    "        tens = pd.to_numeric(df[f\"{DCGM_PREFIX}tensor_active\"], errors=\"coerce\")\n",
    "        dram = pd.to_numeric(df[f\"{DCGM_PREFIX}dram_active\"], errors=\"coerce\")\n",
    "\n",
    "        missing_any = fp16.isna() | fp32.isna() | fp64.isna() | tens.isna() | dram.isna()\n",
    "        bad_gt1 = (fp16 > 1.0) | (fp32 > 1.0) | (fp64 > 1.0) | (tens > 1.0) | (dram > 1.0)\n",
    "        all_fp_zero = (fp16.eq(0.0)) & (fp32.eq(0.0)) & (fp64.eq(0.0)) & (tens.eq(0.0))\n",
    "\n",
    "        keep = ~(bad_gt1 | all_fp_zero)\n",
    "        if not keep.any():\n",
    "            continue\n",
    "\n",
    "        job  = job.loc[keep].to_numpy()\n",
    "        host = host.loc[keep].to_numpy()\n",
    "        gpu  = gpu.loc[keep].to_numpy()\n",
    "        ts   = ts.loc[keep].to_numpy()\n",
    "\n",
    "        fp16 = fp16.loc[keep].to_numpy(dtype=float, copy=False)\n",
    "        fp32 = fp32.loc[keep].to_numpy(dtype=float, copy=False)\n",
    "        fp64 = fp64.loc[keep].to_numpy(dtype=float, copy=False)\n",
    "        tens = tens.loc[keep].to_numpy(dtype=float, copy=False)\n",
    "        dram = dram.loc[keep].to_numpy(dtype=float, copy=False)\n",
    "        missing_any = missing_any.loc[keep].to_numpy(dtype=bool, copy=False)\n",
    "\n",
    "        is80 = np.fromiter((h in nodes80 for h in host), dtype=bool, count=len(host))\n",
    "        peak_hbm = np.where(is80, HBM_80, HBM_40)\n",
    "        achieved_hbm = dram * peak_hbm\n",
    "\n",
    "        ridge_fp64 = PEAK_FLOPS_FP64_VECTOR / peak_hbm\n",
    "        achieved_fp64 = fp64 * PEAK_FLOPS_FP64_VECTOR\n",
    "\n",
    "        ai_fp64 = np.full(len(job), np.nan, dtype=float)\n",
    "        np.divide(achieved_fp64, achieved_hbm, out=ai_fp64, where=(achieved_hbm > 0))\n",
    "\n",
    "        any_compute = (fp16 > 0) | (fp32 > 0) | (fp64 > 0) | (tens > 0)\n",
    "        eligible = ~missing_any\n",
    "\n",
    "        labels = np.full(len(job), \"Other\", dtype=object)\n",
    "\n",
    "        mask_inf = eligible & any_compute & (achieved_hbm <= 0)\n",
    "        labels[mask_inf] = \"Compute-intensive\"\n",
    "\n",
    "        mask_pos = eligible & (achieved_hbm > 0) & np.isfinite(ai_fp64) & np.isfinite(ridge_fp64)\n",
    "        mem = mask_pos & (ai_fp64 < ridge_fp64)\n",
    "        comp = mask_pos & ~mem\n",
    "        labels[mem] = \"Memory-intensive\"\n",
    "        labels[comp] = \"Compute-intensive\"\n",
    "\n",
    "        for j, h, g, t, lab in zip(job, host, gpu, ts, labels):\n",
    "            job_samples[j] += 1\n",
    "            if not np.isfinite(g) or not np.isfinite(t):\n",
    "                continue\n",
    "            key = (j, h, int(g))\n",
    "            t = int(t)\n",
    "\n",
    "            if key in state:\n",
    "                last_ts, last_lab = state[key]\n",
    "                dt = t - last_ts\n",
    "                if dt > 0:\n",
    "                    job_time[j][last_lab] += dt\n",
    "                    job_total[j] += dt\n",
    "\n",
    "            state[key] = (t, lab)\n",
    "\n",
    "    rows = []\n",
    "    for j, tot in job_total.items():\n",
    "        if tot <= 0:\n",
    "            continue\n",
    "        c = job_time[j][\"Compute-intensive\"]\n",
    "        m = job_time[j][\"Memory-intensive\"]\n",
    "        o = job_time[j][\"Other\"]\n",
    "        rows.append({\n",
    "            \"JobID\": j,\n",
    "            \"time_seconds\": float(tot) / 1e9,\n",
    "            \"frac_time_compute_fp64only\": c / tot,\n",
    "            \"frac_time_memory_fp64only\":  m / tot,\n",
    "            \"frac_time_other_fp64only\":   o / tot,\n",
    "            \"sample_count\": int(job_samples.get(j, 0)),\n",
    "        })\n",
    "\n",
    "    out = pd.DataFrame(rows).sort_values(\"JobID\")\n",
    "    out.to_parquet(OUT_LABELS, index=False)\n",
    "\n",
    "def bin_labels(edges):\n",
    "    labs = []\n",
    "    for i in range(len(edges) - 1):\n",
    "        a, b = int(np.floor(edges[i])), int(np.ceil(edges[i + 1]))\n",
    "        labs.append(f\"[{a},{b})\" if i < len(edges) - 2 else f\"[{a},{b}]\")\n",
    "    return labs\n",
    "\n",
    "def build_and_plot(joined, job_ids, title_prefix, save_name):\n",
    "    df = joined[joined[\"jobid\"].isin(job_ids)].copy()\n",
    "    x = df[\"x_pct\"].to_numpy()\n",
    "    y = df[\"y_pct\"].to_numpy()\n",
    "    z = df[\"mean_gpu_util\"].to_numpy()\n",
    "\n",
    "    x_edges = np.linspace(0.0, 100.0, 11)\n",
    "    y_edges = np.linspace(0.0, 100.0, 11)\n",
    "\n",
    "    hist_sum, _, _ = np.histogram2d(x, y, bins=[x_edges, y_edges], weights=z, density=False)\n",
    "    counts,   _, _ = np.histogram2d(x, y, bins=[x_edges, y_edges], density=False)\n",
    "    mean_grid = np.divide(hist_sum, counts, out=np.full_like(hist_sum, np.nan), where=counts > 0)\n",
    "\n",
    "    x_labs = bin_labels(x_edges)\n",
    "    y_labs = bin_labels(y_edges)\n",
    "    mean_df = pd.DataFrame(mean_grid.T, index=y_labs, columns=x_labs)\n",
    "    cnt_df  = pd.DataFrame(counts.T,    index=y_labs, columns=x_labs)\n",
    "\n",
    "    pd.set_option(\"display.max_columns\", None)\n",
    "    pd.set_option(\"display.width\", 220)\n",
    "\n",
    "    setup_local()\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(\n",
    "        mean_grid.T, origin=\"lower\", aspect=\"auto\",\n",
    "        extent=[0.0, 100.0, 0.0, 100.0],\n",
    "        cmap=\"Blues\", vmin=0, vmax=100\n",
    "    )\n",
    "    cbar = plt.colorbar(label=\"\")\n",
    "    cbar.ax.tick_params(labelsize=36)\n",
    "\n",
    "    plt.xlabel(\"Mean (of FP64_ACTV) %\", fontsize=38)\n",
    "    plt.ylabel(\"Mean (of DRAM_ACTV) %\", fontsize=38)\n",
    "    plt.xticks([0, 20, 40, 60, 80, 100], fontsize=36)\n",
    "    plt.yticks([0, 20, 40, 60, 80, 100], fontsize=36)\n",
    "    plt.title(title_prefix, pad=16, fontsize=40)\n",
    "\n",
    "    orange_cmap = ListedColormap([\"#FF8C00\"])\n",
    "    plt.imshow(\n",
    "        np.where(np.isnan(mean_grid.T), -1, np.nan),\n",
    "        origin=\"lower\", aspect=\"auto\",\n",
    "        extent=[0.0, 100.0, 0.0, 100.0],\n",
    "        cmap=orange_cmap, vmin=-1, vmax=0, alpha=1\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_fp64_dram_heatmaps():\n",
    "    os.makedirs(\"pssg-plots\", exist_ok=True)\n",
    "\n",
    "    labels = pd.read_parquet(\n",
    "        OUT_LABELS,\n",
    "        columns=[\"JobID\", \"frac_time_compute_fp64only\", \"frac_time_memory_fp64only\"]\n",
    "    ).astype({\"JobID\": str})\n",
    "\n",
    "    comp_mask = labels[\"frac_time_compute_fp64only\"] > labels[\"frac_time_memory_fp64only\"]\n",
    "    mem_mask  = labels[\"frac_time_memory_fp64only\"]  > labels[\"frac_time_compute_fp64only\"]\n",
    "    compute_jobs = labels.loc[comp_mask, \"JobID\"]\n",
    "    memory_jobs  = labels.loc[mem_mask,  \"JobID\"]\n",
    "\n",
    "    means = pd.read_parquet(\n",
    "        OUT_MEANS,\n",
    "        columns=[\"JobID\", \"nersc_ldms_dcgm_fp64_active\", \"nersc_ldms_dcgm_dram_active\"],\n",
    "    ).rename(columns={\n",
    "        \"JobID\": \"jobid\",\n",
    "        \"nersc_ldms_dcgm_fp64_active\": \"fp64\",\n",
    "        \"nersc_ldms_dcgm_dram_active\": \"dram\",\n",
    "    })\n",
    "    means[\"jobid\"] = means[\"jobid\"].astype(str)\n",
    "    means[\"x_pct\"] = 100.0 * means[\"fp64\"]\n",
    "    means[\"y_pct\"] = 100.0 * means[\"dram\"]\n",
    "\n",
    "    metrics = pd.read_parquet(\n",
    "        OUT_METRICS,\n",
    "        columns=[\"JobID\", \"mean_utilization\"]\n",
    "    ).rename(columns={\"JobID\": \"jobid\", \"mean_utilization\": \"mean_gpu_util\"})\n",
    "    metrics[\"jobid\"] = metrics[\"jobid\"].astype(str)\n",
    "\n",
    "    joined = means.merge(metrics, on=\"jobid\", how=\"inner\")\n",
    "\n",
    "    build_and_plot(joined, compute_jobs, \"Compute-bound jobs\\nMean (of GPU_UTIL) %\", \"comp_fp64_dram_gputil.pdf\")\n",
    "    build_and_plot(joined, memory_jobs,  \"Memory-bound jobs\\nMean (of GPU_UTIL) %\", \"mem_fp64_dram_gputil.pdf\")\n",
    "\n",
    "build_job_means_all()\n",
    "build_job_metrics_mean_util()\n",
    "build_fp64only_label_fractions()\n",
    "plot_fp64_dram_heatmaps()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
