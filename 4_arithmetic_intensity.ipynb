{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8b113c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "from setup_plot import setup_local, get_colors\n",
    "\n",
    "\n",
    "INPUT_SET1      = \"ldms.parquet\"\n",
    "BATCH_SIZE      = 250_000\n",
    "\n",
    "NODES_80GB_FILE = Path(\"nodes_80gb.txt\")\n",
    "SACCT_CSV       = Path(\"sacct.csv\") \n",
    "\n",
    "PLOTS = [\"fp32\", \"fp64vec\", \"tensor\", \"pseudo64\"]\n",
    "\n",
    "AI_MIN, AI_MAX = 1e-3, 1e3\n",
    "BARS_PER_DECADE = 5\n",
    "\n",
    "X_TICKS = [1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3]\n",
    "Y_TICKS_LEFT  = [0, 5, 10, 15, 20, 25]\n",
    "Y_TICKS_RIGHT = [1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2]\n",
    "\n",
    "# DCGM columns\n",
    "DCGM_PREFIX = \"nersc_ldms_dcgm_\"\n",
    "ACTIVE_COLS = [\n",
    "    f\"{DCGM_PREFIX}fp16_active\",\n",
    "    f\"{DCGM_PREFIX}fp32_active\",\n",
    "    f\"{DCGM_PREFIX}fp64_active\",\n",
    "    f\"{DCGM_PREFIX}tensor_active\",\n",
    "    f\"{DCGM_PREFIX}dram_active\",\n",
    "]\n",
    "NEEDED_COLS: List[str] = [\"JobID\", \"hostname\"] + ACTIVE_COLS\n",
    "\n",
    "# A100 peaks (FLOP/s)\n",
    "PEAK_FLOPS_FP16        = 78.0e12\n",
    "PEAK_FLOPS_FP32        = 19.5e12\n",
    "PEAK_FLOPS_FP64_VECTOR = 9.7e12\n",
    "PEAK_FLOPS_FP64_TENSOR = 19.5e12\n",
    "\n",
    "# HBM roofs (B/s)\n",
    "HBM_40 = 1.555e12\n",
    "HBM_80 = 2.039e12\n",
    "\n",
    "# Plot ridge/roofline constants (A100)\n",
    "PEAKS_40 = {\"FP32\": PEAK_FLOPS_FP32, \"FP64\": PEAK_FLOPS_FP64_VECTOR, \"TNSR\": PEAK_FLOPS_FP64_TENSOR, \"HBM\": HBM_40}\n",
    "PEAKS_80 = {\"FP32\": PEAK_FLOPS_FP32, \"FP64\": PEAK_FLOPS_FP64_VECTOR, \"TNSR\": PEAK_FLOPS_FP64_TENSOR, \"HBM\": HBM_80}\n",
    "RIDGE_40 = {\"FP32\": PEAKS_40[\"FP32\"]/HBM_40, \"FP64\": PEAKS_40[\"FP64\"]/HBM_40, \"TNSR\": PEAKS_40[\"TNSR\"]/HBM_40, \"HBM\": HBM_40}\n",
    "RIDGE_80 = {\"FP32\": PEAKS_80[\"FP32\"]/HBM_80, \"FP64\": PEAKS_80[\"FP64\"]/HBM_80, \"TNSR\": PEAKS_80[\"TNSR\"]/HBM_80, \"HBM\": HBM_80}\n",
    "\n",
    "# Which AI column corresponds to each plot\n",
    "AI_COLS = {\n",
    "    \"fp32\":     \"AI_fp32\",\n",
    "    \"fp64vec\":  \"AI_fp64_vector\",\n",
    "    \"tensor\":   \"AI_tensor_fp64\",\n",
    "    \"pseudo64\": \"pseudo64_AI_tensor_fp64\",\n",
    "}\n",
    "\n",
    "\n",
    "def load_nodes_from_file(path: Path) -> set[str]:\n",
    "    \"\"\"Read hostnames from a file (one per line).\"\"\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"80GB node list not found: {path.resolve()}\")\n",
    "    nodes: set[str] = set()\n",
    "    for ln in path.read_text(encoding=\"utf-8\", errors=\"replace\").splitlines():\n",
    "        ln = ln.strip()\n",
    "        if not ln or ln.startswith(\"#\"):\n",
    "            continue\n",
    "        for tok in ln.replace(\",\", \" \").split():\n",
    "            tok = tok.strip()\n",
    "            if tok and not tok.startswith(\"#\"):\n",
    "                nodes.add(tok)\n",
    "    return nodes\n",
    "\n",
    "\n",
    "def read_pipe_csv(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"CSV read (SubmitLine may contain '|').\"\"\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"sacct file not found: {path.resolve()}\")\n",
    "\n",
    "    with path.open(\"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        header = f.readline().rstrip(\"\\n\")\n",
    "    names = header.split(\"|\")\n",
    "    ncols = len(names)\n",
    "\n",
    "    rows = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        next(f) \n",
    "        for ln in f:\n",
    "            parts = ln.rstrip(\"\\n\").split(\"|\", ncols - 1)\n",
    "            if len(parts) < ncols:\n",
    "                parts += [\"\"] * (ncols - len(parts))\n",
    "            rows.append(parts)\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=names)\n",
    "    df[\"JobID\"] = df[\"JobID\"].astype(str)\n",
    "    return df\n",
    "\n",
    "\n",
    "def requested_hbm_from_constraints(s: str):\n",
    "    s = (s or \"\").lower()\n",
    "    toks = [t.strip() for t in re.split(r\"[&,+]\", s) if t.strip()]\n",
    "    if \"hbm80g\" in toks:\n",
    "        return \"requested_80gb\"\n",
    "    if \"hbm40g\" in toks:\n",
    "        return \"requested_40gb\"\n",
    "    return \"unspecified\"\n",
    "\n",
    "\n",
    "def requested_hbm_from_submitline(s: str):\n",
    "    s = (s or \"\")\n",
    "    m = re.search(r\"(?:-C|--constraint)\\s*=?\\s*([^\\s]+)\", s)\n",
    "    return requested_hbm_from_constraints(m.group(1)) if m else \"unspecified\"\n",
    "\n",
    "\n",
    "def load_req80_ids(sacct_csv: Path) -> set[str]:\n",
    "    \"\"\"Return JobIDs that explicitly requested 80GB (hbm80g) using Constraints + SubmitLine.\"\"\"\n",
    "    df = read_pipe_csv(sacct_csv)\n",
    "\n",
    "    if \"Constraints\" in df.columns:\n",
    "        df[\"requested_gpu_mem\"] = df[\"Constraints\"].apply(requested_hbm_from_constraints)\n",
    "    else:\n",
    "        df[\"requested_gpu_mem\"] = \"unspecified\"\n",
    "\n",
    "    if \"SubmitLine\" in df.columns:\n",
    "        mask_unspec = df[\"requested_gpu_mem\"].eq(\"unspecified\")\n",
    "        df.loc[mask_unspec, \"requested_gpu_mem\"] = df.loc[mask_unspec, \"SubmitLine\"].apply(requested_hbm_from_submitline)\n",
    "\n",
    "    req80 = df.loc[df[\"requested_gpu_mem\"].eq(\"requested_80gb\"), \"JobID\"].astype(str).unique().tolist()\n",
    "    return set(req80)\n",
    "\n",
    "\n",
    "def make_log_edges(ai_min: float, ai_max: float, bars_per_decade: int) -> np.ndarray:\n",
    "    lo = np.log10(ai_min)\n",
    "    hi = np.log10(ai_max)\n",
    "    nbins = int(round((hi - lo) * bars_per_decade))\n",
    "    return np.logspace(lo, hi, nbins + 1)\n",
    "\n",
    "\n",
    "def hist_from_parquet_edges(parquet_path: str, ai_col: str, capacity: float,\n",
    "                            edges: np.ndarray, nodes80: set[str],\n",
    "                            req_jobids: set[str] | None = None) -> Tuple[np.ndarray, int]:\n",
    "    pf = pq.ParquetFile(parquet_path)\n",
    "    present = set(pf.schema_arrow.names)\n",
    "    missing = [c for c in NEEDED_COLS if c not in present]\n",
    "    if missing:\n",
    "        raise KeyError(f\"Missing required columns in {parquet_path}: {missing}\")\n",
    "\n",
    "    counts = np.zeros(len(edges) - 1, dtype=float)\n",
    "    total_inrange = 0\n",
    "\n",
    "    for batch in pf.iter_batches(columns=NEEDED_COLS, batch_size=BATCH_SIZE):\n",
    "        df = batch.to_pandas()\n",
    "        if len(df) == 0:\n",
    "            continue\n",
    "\n",
    "        fp16 = pd.to_numeric(df[f\"{DCGM_PREFIX}fp16_active\"], errors=\"coerce\")\n",
    "        fp32 = pd.to_numeric(df[f\"{DCGM_PREFIX}fp32_active\"], errors=\"coerce\")\n",
    "        fp64 = pd.to_numeric(df[f\"{DCGM_PREFIX}fp64_active\"], errors=\"coerce\")\n",
    "        tens = pd.to_numeric(df[f\"{DCGM_PREFIX}tensor_active\"], errors=\"coerce\")\n",
    "        dram = pd.to_numeric(df[f\"{DCGM_PREFIX}dram_active\"], errors=\"coerce\")\n",
    "\n",
    "        missing_any = fp16.isna() | fp32.isna() | fp64.isna() | tens.isna() | dram.isna()\n",
    "        bad_gt1 = (fp16 > 1.0) | (fp32 > 1.0) | (fp64 > 1.0) | (tens > 1.0) | (dram > 1.0)\n",
    "        all_fp_zero = (fp16.eq(0.0)) & (fp32.eq(0.0)) & (fp64.eq(0.0)) & (tens.eq(0.0))\n",
    "\n",
    "        keep = ~(missing_any | bad_gt1 | all_fp_zero)\n",
    "        if not keep.any():\n",
    "            continue\n",
    "\n",
    "        df = df.loc[keep].copy()\n",
    "        fp16 = fp16.loc[keep].astype(float)\n",
    "        fp32 = fp32.loc[keep].astype(float)\n",
    "        fp64 = fp64.loc[keep].astype(float)\n",
    "        tens = tens.loc[keep].astype(float)\n",
    "        dram = dram.loc[keep].astype(float)\n",
    "\n",
    "        host = df[\"hostname\"].astype(str)\n",
    "        is80 = host.isin(nodes80).to_numpy()\n",
    "        capacity_gib = np.where(is80, 80.0, 40.0)\n",
    "\n",
    "        mask = np.isfinite(capacity_gib) & np.isclose(capacity_gib, capacity, atol=0.6)\n",
    "\n",
    "        if req_jobids is not None:\n",
    "            mask &= df[\"JobID\"].astype(str).isin(req_jobids).to_numpy()\n",
    "\n",
    "        if not mask.any():\n",
    "            continue\n",
    "\n",
    "        peak_hbm_bps = np.where(is80, HBM_80, HBM_40)\n",
    "\n",
    "        flops_fp16 = fp16.to_numpy() * PEAK_FLOPS_FP16\n",
    "        flops_fp32 = fp32.to_numpy() * PEAK_FLOPS_FP32\n",
    "        flops_fp64 = fp64.to_numpy() * PEAK_FLOPS_FP64_VECTOR\n",
    "        flops_tens_fp64 = tens.to_numpy() * PEAK_FLOPS_FP64_TENSOR\n",
    "        hbm_bps = dram.to_numpy() * peak_hbm_bps\n",
    "\n",
    "        mask_hbm = hbm_bps > 0\n",
    "\n",
    "        vals_ai = np.full(len(df), np.nan, dtype=float)\n",
    "        if ai_col == \"AI_fp32\":\n",
    "            np.divide(flops_fp32, hbm_bps, out=vals_ai, where=mask_hbm)\n",
    "        elif ai_col == \"AI_fp64_vector\":\n",
    "            np.divide(flops_fp64, hbm_bps, out=vals_ai, where=mask_hbm)\n",
    "        elif ai_col == \"AI_tensor_fp64\":\n",
    "            np.divide(flops_tens_fp64, hbm_bps, out=vals_ai, where=mask_hbm)\n",
    "        elif ai_col == \"pseudo64_AI_tensor_fp64\":\n",
    "            pseudo64_flops = 0.25*flops_fp16 + 0.5*flops_fp32 + (flops_fp64 + flops_tens_fp64)\n",
    "            np.divide(pseudo64_flops, hbm_bps, out=vals_ai, where=mask_hbm)\n",
    "        else:\n",
    "            raise KeyError(f\"Unknown ai_col: {ai_col}\")\n",
    "\n",
    "        vals = vals_ai[mask]\n",
    "        vals = vals[np.isfinite(vals)]\n",
    "        if vals.size == 0:\n",
    "            continue\n",
    "\n",
    "        vals = vals[(vals > 0) & (vals >= AI_MIN) & (vals <= AI_MAX)]\n",
    "        if vals.size == 0:\n",
    "            continue\n",
    "\n",
    "        h, _ = np.histogram(vals, bins=edges)\n",
    "        counts += h\n",
    "        total_inrange += int(vals.size)\n",
    "\n",
    "    return counts, total_inrange\n",
    "\n",
    "\n",
    "def plot_ai_overlay(parquet_path: str, col_key: str, title: str, ridge_key: str, req80_ids: set[str], nodes80: set[str]):\n",
    "    setup_local()\n",
    "    colors = get_colors()\n",
    "\n",
    "    edges = make_log_edges(AI_MIN, AI_MAX, BARS_PER_DECADE)\n",
    "    lefts  = edges[:-1]\n",
    "    widths = edges[1:] - edges[:-1]\n",
    "\n",
    "    ai_col = AI_COLS[col_key]\n",
    "\n",
    "    c40, n40 = hist_from_parquet_edges(parquet_path, ai_col, 40.0, edges, nodes80=nodes80, req_jobids=None)\n",
    "    if n40 == 0:\n",
    "        print(f\"[info] no 40GB in-range samples for {col_key}\")\n",
    "        return\n",
    "    pct40 = (c40 / n40) * 100.0\n",
    "\n",
    "    c80, n80 = hist_from_parquet_edges(parquet_path, ai_col, 80.0, edges, nodes80=nodes80, req_jobids=req80_ids)\n",
    "    pct80 = (c80 / n80) * 100.0 if n80 > 0 else np.zeros_like(pct40)\n",
    "\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    ax1.bar(lefts, pct40, width=widths, align=\"edge\", color=colors[2], alpha=0.50, edgecolor=\"black\", label=\"40 GB\")\n",
    "    if n80 > 0:\n",
    "        ax1.bar(lefts, pct80, width=widths, align=\"edge\", color=colors[0], alpha=0.50, edgecolor=\"black\", label=\"80 GB (requested 80)\")\n",
    "\n",
    "    ax1.set_xscale(\"log\")\n",
    "    ax1.set_xlim(AI_MIN, AI_MAX)\n",
    "    ax1.set_xticks(X_TICKS)\n",
    "    ax1.set_xlabel(\"Arithmetic Intensity (FLOP/Byte)\", fontsize=18)\n",
    "    ax1.set_ylabel(\"Fraction of samples (%)\", fontsize=18)\n",
    "    ax1.set_yticks(Y_TICKS_LEFT)\n",
    "    ax1.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    ax1.axvline(RIDGE_40[ridge_key], linestyle=\"--\", color=colors[2])\n",
    "    ax1.axvline(RIDGE_80[ridge_key], linestyle=\"--\", color=colors[0])\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    x = np.logspace(-3, 3, 512)\n",
    "    perf40 = np.minimum(x * RIDGE_40[\"HBM\"], PEAKS_40[ridge_key]) / 1e12 \n",
    "    perf80 = np.minimum(x * RIDGE_80[\"HBM\"], PEAKS_80[ridge_key]) / 1e12\n",
    "\n",
    "    ax2.plot(x, perf40, color=colors[2], linewidth=2)\n",
    "    ax2.plot(x, perf80, color=colors[0], linewidth=2, alpha=0.85)\n",
    "    ax2.set_xscale(\"log\")\n",
    "    ax2.set_yscale(\"log\")\n",
    "    ax2.set_ylabel(\"Performance roofline (TF/s)\", fontsize=18)\n",
    "    ax2.set_yticks(Y_TICKS_RIGHT)\n",
    "    ax2.set_xticks(X_TICKS)\n",
    "\n",
    "    ax1.tick_params(axis=\"x\", labelsize=14)\n",
    "    ax1.tick_params(axis=\"y\", labelsize=14)\n",
    "    ax2.tick_params(axis=\"y\", labelsize=14)\n",
    "\n",
    "    ax1.legend(loc=\"upper left\", frameon=True, fontsize=13, framealpha=0.5)\n",
    "    plt.title(title, fontsize=19, pad=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    assert np.isclose(pct40.sum(), 100.0, atol=1e-3)\n",
    "    if n80 > 0:\n",
    "        assert np.isclose(pct80.sum(), 100.0, atol=1e-3)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    nodes_80gb_set = load_nodes_from_file(NODES_80GB_FILE)\n",
    "\n",
    "    req80_ids = load_req80_ids(SACCT_CSV)\n",
    "\n",
    "    if \"fp64vec\" in PLOTS:\n",
    "        plot_ai_overlay(INPUT_SET1, \"fp64vec\", \"Distribution of FP64 AI\", ridge_key=\"FP64\", req80_ids=req80_ids, nodes80=nodes_80gb_set)\n",
    "    if \"tensor\" in PLOTS:\n",
    "        plot_ai_overlay(INPUT_SET1, \"tensor\", \"Distribution of Tensor (FP64-tensor) AI\", ridge_key=\"TNSR\", req80_ids=req80_ids, nodes80=nodes_80gb_set)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
