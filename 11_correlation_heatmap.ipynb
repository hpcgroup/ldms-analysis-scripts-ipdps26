{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf82a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import duckdb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "SET1 = \"ldms_set1.parquet\"\n",
    "SET2 = \"ldms_set2.parquet\"\n",
    "\n",
    "SACCT_CSV = \"slurm.csv\"\n",
    "SACCT_TZ  = \"US/Pacific\"  \n",
    "\n",
    "MEANS_PATH   = \"job_means_all.parquet\"\n",
    "METRICS_PATH = \"job_metrics.parquet\"\n",
    "ENERGY_PATH  = \"energy_per_job.parquet\"\n",
    "\n",
    "OUT_DIR = \"corr\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "def list_counters(fname: str) -> list[str]:\n",
    "    con = duckdb.connect()\n",
    "    try:\n",
    "        cols = con.execute(f\"DESCRIBE SELECT * FROM parquet_scan('{fname}')\").fetchall()\n",
    "    finally:\n",
    "        con.close()\n",
    "    return [c[0] for c in cols if c[0].startswith(\"nersc_ldms_dcgm_\")]\n",
    "\n",
    "def avg_expr(cols: list[str], overlap: set[str], suffix_overlaps: bool = False) -> str:\n",
    "    parts = []\n",
    "    for c in cols:\n",
    "        alias = f\"{c}_s2\" if (suffix_overlaps and c in overlap) else c\n",
    "        parts.append(f\"avg({c}) AS {alias}\")\n",
    "    return \",\\n       \".join(parts)\n",
    "\n",
    "def build_job_means_all():\n",
    "    counters1 = list_counters(SET1)\n",
    "    counters2 = list_counters(SET2)\n",
    "    overlap = set(counters1).intersection(counters2)\n",
    "\n",
    "    con = duckdb.connect()\n",
    "    con.execute(\"PRAGMA memory_limit='15GB';\")\n",
    "    con.execute(f\"PRAGMA threads={os.cpu_count() or 1};\")\n",
    "\n",
    "    con.execute(f\"\"\"\n",
    "    COPY (\n",
    "      WITH\n",
    "      s1_gpu AS (\n",
    "        SELECT\n",
    "          JobID::VARCHAR AS JobID,\n",
    "          hostname,\n",
    "          gpu_id,\n",
    "          {avg_expr(counters1, overlap, suffix_overlaps=False)}\n",
    "        FROM parquet_scan('{SET1}')\n",
    "        GROUP BY JobID, hostname, gpu_id\n",
    "      ),\n",
    "      s1_job AS (\n",
    "        SELECT\n",
    "          JobID,\n",
    "          {avg_expr(counters1, overlap, suffix_overlaps=False)}\n",
    "        FROM s1_gpu\n",
    "        GROUP BY JobID\n",
    "      ),\n",
    "      s2_gpu AS (\n",
    "        SELECT\n",
    "          JobID::VARCHAR AS JobID,\n",
    "          hostname,\n",
    "          gpu_id,\n",
    "          {avg_expr(counters2, overlap, suffix_overlaps=False)}\n",
    "        FROM parquet_scan('{SET2}')\n",
    "        GROUP BY JobID, hostname, gpu_id\n",
    "      ),\n",
    "      s2_job AS (\n",
    "        SELECT\n",
    "          JobID,\n",
    "          {avg_expr(counters2, overlap, suffix_overlaps=True)}\n",
    "        FROM s2_gpu\n",
    "        GROUP BY JobID\n",
    "      )\n",
    "      SELECT *\n",
    "      FROM s1_job AS s1\n",
    "      LEFT JOIN s2_job AS s2 USING (JobID)\n",
    "    )\n",
    "    TO '{MEANS_PATH}' (FORMAT PARQUET, COMPRESSION 'SNAPPY');\n",
    "    \"\"\")\n",
    "\n",
    "    con.close()\n",
    "\n",
    "def build_job_metrics():\n",
    "    con = duckdb.connect()\n",
    "    con.execute(\"PRAGMA memory_limit='15GB';\")\n",
    "    con.execute(f\"PRAGMA threads={os.cpu_count() or 1};\")\n",
    "\n",
    "    ng = con.execute(f\"\"\"\n",
    "      SELECT\n",
    "        JobID::VARCHAR AS JobID,\n",
    "        COUNT(*)::BIGINT AS ngpus\n",
    "      FROM (\n",
    "        SELECT DISTINCT JobID, hostname, gpu_id\n",
    "        FROM parquet_scan('{SET1}')\n",
    "      )\n",
    "      GROUP BY JobID\n",
    "    \"\"\").df()\n",
    "    con.close()\n",
    "\n",
    "    sacct = pd.read_csv(SACCT_CSV, usecols=[c for c in [\"JobID\",\"Start\",\"End\",\"npus\"] if c in pd.read_csv(SACCT_CSV, nrows=0).columns], low_memory=False)\n",
    "    sacct[\"JobID\"] = sacct[\"JobID\"].astype(str)\n",
    "\n",
    "    start = pd.to_datetime(sacct[\"Start\"], errors=\"coerce\").dt.tz_localize(SACCT_TZ).dt.tz_convert(\"UTC\")\n",
    "    end   = pd.to_datetime(sacct[\"End\"],   errors=\"coerce\").dt.tz_localize(SACCT_TZ).dt.tz_convert(\"UTC\")\n",
    "    sacct[\"start_time\"] = start\n",
    "    sacct[\"end_time\"]   = end\n",
    "    sacct = sacct.dropna(subset=[\"start_time\",\"end_time\"])\n",
    "\n",
    "    sacct = (sacct.groupby(\"JobID\", as_index=False)\n",
    "                  .agg(start_time=(\"start_time\",\"min\"),\n",
    "                       end_time=(\"end_time\",\"max\"),\n",
    "                       **({\"npus\": (\"npus\",\"max\")} if \"npus\" in sacct.columns else {})))\n",
    "\n",
    "    sacct[\"duration_hours\"] = (sacct[\"end_time\"] - sacct[\"start_time\"]).dt.total_seconds() / 3600.0\n",
    "    sacct[\"duration_hours\"] = sacct[\"duration_hours\"].clip(lower=0)\n",
    "\n",
    "    out = sacct.merge(ng, on=\"JobID\", how=\"inner\")\n",
    "    cols_out = [\"JobID\"]\n",
    "    if \"npus\" in out.columns:\n",
    "        cols_out.append(\"npus\")\n",
    "    cols_out += [\"ngpus\", \"duration_hours\"]\n",
    "\n",
    "    out[cols_out].to_parquet(METRICS_PATH, index=False)\n",
    "\n",
    "def build_energy_per_job(set2_path: str, out_path: str, batch_size: int = 1_000_000):\n",
    "    COL_JOB = \"JobID\"\n",
    "    COL_HOST = \"hostname\"\n",
    "    COL_GPU = \"gpu_id\"\n",
    "    COL_TS = \"ts_ns\"\n",
    "    COL_ENE = \"nersc_ldms_dcgm_total_energy_consumption\"\n",
    "\n",
    "    dataset = ds.dataset(set2_path, format=\"parquet\")\n",
    "    scanner = dataset.scanner(\n",
    "        columns=[COL_JOB, COL_HOST, COL_GPU, COL_TS, COL_ENE],\n",
    "        batch_size=batch_size,\n",
    "        use_threads=True,\n",
    "    )\n",
    "\n",
    "    state = {}\n",
    "    for batch in scanner.to_batches():\n",
    "        pdf = pa.Table.from_batches([batch]).to_pandas(types_mapper=pd.ArrowDtype)\n",
    "\n",
    "        pdf[COL_JOB]  = pdf[COL_JOB].astype(\"string\")\n",
    "        pdf[COL_HOST] = pdf[COL_HOST].astype(\"string\")\n",
    "        pdf[COL_GPU]  = pd.to_numeric(pdf[COL_GPU], errors=\"coerce\").astype(\"Int64\")\n",
    "        pdf[COL_TS]   = pd.to_numeric(pdf[COL_TS],  errors=\"coerce\").astype(\"Int64\")\n",
    "        pdf[COL_ENE]  = pd.to_numeric(pdf[COL_ENE], errors=\"coerce\").astype(\"Float64\")\n",
    "\n",
    "        pdf = pdf.dropna(subset=[COL_TS, COL_ENE, COL_GPU, COL_JOB, COL_HOST])\n",
    "        if pdf.empty:\n",
    "            continue\n",
    "\n",
    "        grp = pdf.groupby([COL_JOB, COL_HOST, COL_GPU], sort=False, observed=True)\n",
    "        counts = grp.size().reset_index(name=\"n\").set_index([COL_JOB, COL_HOST, COL_GPU])\n",
    "\n",
    "        first_rows = (\n",
    "            pdf.drop_duplicates(subset=[COL_JOB, COL_HOST, COL_GPU], keep=\"first\")\n",
    "               .set_index([COL_JOB, COL_HOST, COL_GPU])[[COL_ENE]]\n",
    "               .rename(columns={COL_ENE: \"first_e\"})\n",
    "        )\n",
    "        last_rows = (\n",
    "            pdf.drop_duplicates(subset=[COL_JOB, COL_HOST, COL_GPU], keep=\"last\")\n",
    "               .set_index([COL_JOB, COL_HOST, COL_GPU])[[COL_ENE]]\n",
    "               .rename(columns={COL_ENE: \"last_e\"})\n",
    "        )\n",
    "\n",
    "        batch_keys = set((str(i[0]), str(i[1]), int(i[2])) for i in counts.index)\n",
    "\n",
    "        for (j, h, g) in batch_keys:\n",
    "            fe = float(first_rows.loc[(j, h, g), \"first_e\"])\n",
    "            le = float(last_rows.loc[(j, h, g), \"last_e\"])\n",
    "            key = (j, h, g)\n",
    "            if key not in state:\n",
    "                state[key] = {\"first_e\": fe, \"last_e\": le}\n",
    "            else:\n",
    "                state[key][\"last_e\"] = le\n",
    "\n",
    "    rows_gpu = []\n",
    "    for (j, h, g), st in state.items():\n",
    "        rows_gpu.append((j, st[\"last_e\"] - st[\"first_e\"]))\n",
    "\n",
    "    df_gpu = pd.DataFrame(rows_gpu, columns=[\"JobID\", \"energy_delta\"])\n",
    "    df_job = df_gpu.groupby(\"JobID\", as_index=False).agg(total_energy_joules=(\"energy_delta\", \"sum\"))\n",
    "    df_job.to_parquet(out_path, index=False)\n",
    "\n",
    "def correlation_heatmap():\n",
    "    PREFIX = \"nersc_ldms_dcgm_\"\n",
    "    THR    = 0.5\n",
    "\n",
    "    pf_means   = pq.ParquetFile(MEANS_PATH)\n",
    "    pf_metrics = pq.ParquetFile(METRICS_PATH)\n",
    "\n",
    "    means_cols_all   = pf_means.schema_arrow.names\n",
    "    metrics_cols_all = pf_metrics.schema_arrow.names\n",
    "\n",
    "    counter_cols = [c for c in means_cols_all if c.startswith(PREFIX)]\n",
    "    need_means   = [\"JobID\"] + counter_cols\n",
    "\n",
    "    means = pd.read_parquet(MEANS_PATH, columns=need_means).copy()\n",
    "    means[\"JobID\"] = means[\"JobID\"].astype(str)\n",
    "\n",
    "    rename_map = {c: c[len(PREFIX):] for c in counter_cols}\n",
    "    means = means.rename(columns=rename_map)\n",
    "\n",
    "    drop_means = {\n",
    "        \"fb_free\", \"gr_engine_active\", \"sm_occupancy\", \"tensor_hmma_active\",\n",
    "        \"memory_clock\", \"memory_temp\", \"power_usage\", \"mem_copy_utilization\"\n",
    "    }\n",
    "    means = means.drop(columns=[c for c in drop_means if c in means.columns], errors=\"ignore\")\n",
    "\n",
    "    pretty_map = {\n",
    "        \"dram_active\":        \"DRAM_ACTV\",\n",
    "        \"fb_used\":            \"HBM_USED\",\n",
    "        \"fp16_active\":        \"FP16_ACTV\",\n",
    "        \"fp32_active\":        \"FP32_ACTV\",\n",
    "        \"fp64_active\":        \"FP64_ACTV\",\n",
    "        \"sm_active\":          \"SM_ACTV\",\n",
    "        \"gpu_utilization\":    \"GPU_UTIL\",\n",
    "        \"nvlink_rx_bytes\":    \"NVLINK_RX\",\n",
    "        \"nvlink_tx_bytes\":    \"NVLINK_TX\",\n",
    "        \"pcie_rx_bytes\":      \"PCIE_RX\",\n",
    "        \"pcie_tx_bytes\":      \"PCIE_TX\",\n",
    "        \"gpu_temp\":           \"GPU_TEMP\",\n",
    "        \"tensor_active\":      \"TNSR_ACTV\",\n",
    "    }\n",
    "    means = means.rename(columns=pretty_map)\n",
    "\n",
    "    extras_candidates = [\"JobID\", \"npus\", \"ngpus\", \"duration_hours\"]\n",
    "    extras_present    = [c for c in extras_candidates if c in metrics_cols_all]\n",
    "    metrics = pd.read_parquet(METRICS_PATH, columns=extras_present).copy()\n",
    "    if \"JobID\" in metrics.columns:\n",
    "        metrics[\"JobID\"] = metrics[\"JobID\"].astype(str)\n",
    "\n",
    "    energy = pd.read_parquet(ENERGY_PATH, columns=[\"JobID\", \"total_energy_joules\"]).copy()\n",
    "    energy[\"JobID\"] = energy[\"JobID\"].astype(str)\n",
    "\n",
    "    df = means.merge(metrics, on=\"JobID\", how=\"left\").merge(energy, on=\"JobID\", how=\"left\")\n",
    "\n",
    "    for c in [\"total_energy_joules\", \"ngpus\", \"duration_hours\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    den = df.get(\"ngpus\", np.nan) * df.get(\"duration_hours\", np.nan) * 3600.0\n",
    "    df[\"power_per_gpu\"] = df[\"total_energy_joules\"] / den\n",
    "    df = df.rename(columns={\"power_per_gpu\": \"GPU_POWER\"})\n",
    "\n",
    "    drop_from_num = [\"JobID\", \"total_energy_consumption\", \"total_energy_joules\", \"ngpus\", \"duration_hours\"]\n",
    "    num = df.drop(columns=[c for c in drop_from_num if c in df.columns], errors=\"ignore\")\n",
    "\n",
    "    for c in num.columns:\n",
    "        num[c] = pd.to_numeric(num[c], errors=\"coerce\")\n",
    "\n",
    "    std = num.std(numeric_only=True, ddof=0)\n",
    "    keep_cols = std[std > 0].index.tolist()\n",
    "    num = num[keep_cols]\n",
    "\n",
    "    corr = num.corr(method=\"spearman\")\n",
    "\n",
    "    sel = corr.abs() >= THR\n",
    "    ann = corr.where(sel).round(2).astype(object)\n",
    "    ann[~sel] = \"\"\n",
    "\n",
    "    order = corr[\"GPU_UTIL\"].abs().sort_values(ascending=False).index.tolist()\n",
    "    corr = corr.loc[order, order]\n",
    "    ann  = ann.loc[order,  order]\n",
    "\n",
    "    n_cols = corr.shape[1]\n",
    "    fig_w = max(14, min(0.8 * n_cols, 60))\n",
    "    fig_h = max(12, min(0.8 * n_cols, 60))\n",
    "\n",
    "    plt.figure(figsize=(fig_w, fig_h))\n",
    "    ax = sns.heatmap(\n",
    "        corr,\n",
    "        annot=ann, fmt=\"\",\n",
    "        cmap=\"coolwarm\",\n",
    "        vmin=-1, vmax=1,\n",
    "        linewidths=2,\n",
    "        linecolor=\"black\",\n",
    "        cbar=True,\n",
    "        annot_kws={\"size\": 26, \"color\": \"white\"},\n",
    "    )\n",
    "\n",
    "    plt.xticks(rotation=90, fontsize=28)\n",
    "    plt.yticks(rotation=0, fontsize=28)\n",
    "    cbar = ax.collections[0].colorbar\n",
    "    cbar.ax.tick_params(labelsize=28)\n",
    "\n",
    "    plt.title(\"Correlation Heatmap of Mean Counter Values\", fontsize=30, pad=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return corr\n",
    "\n",
    "build_job_means_all()\n",
    "build_job_metrics()\n",
    "build_energy_per_job(SET2, ENERGY_PATH)\n",
    "corr = correlation_heatmap()\n",
    "corr\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
