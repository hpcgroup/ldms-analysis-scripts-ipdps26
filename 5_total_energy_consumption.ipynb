{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5168c50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FixedLocator, LogFormatterMathtext, NullLocator\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "from setup_plot import setup_local, get_colors, get_markers\n",
    "\n",
    "\n",
    "# SET1 contains: JobID, hostname, gpu_id, ts_ns, and the *_active counters\n",
    "SET1_PARQUET = \"ldms_set1.parquet\"\n",
    "\n",
    "# SET2 contains:\n",
    "#   nersc_ldms_dcgm_total_energy_consumption\n",
    "SET2_PARQUET = \"ldms_set2.parquet\"\n",
    "\n",
    "SACCT_CSV = \"slurm.csv\"\n",
    "NODES_80GB_FILE = \"nodes_80gb.txt\"\n",
    "\n",
    "LABELS_FILE = \"job_label_fractions_fp64only.parquet\"\n",
    "JOB_METRICS_FILE = \"job_metrics.parquet\"\n",
    "ENERGY_PER_JOB = \"energy_per_job.parquet\"\n",
    "\n",
    "BATCH_SET1 = 1_000_000\n",
    "BATCH_SET2 = 1_000_000\n",
    "\n",
    "SACCT_TZ = \"US/Pacific\"\n",
    "\n",
    "DCGM_PREFIX = \"nersc_ldms_dcgm_\"\n",
    "ACTIVE_COLS = [\n",
    "    f\"{DCGM_PREFIX}fp16_active\",\n",
    "    f\"{DCGM_PREFIX}fp32_active\",\n",
    "    f\"{DCGM_PREFIX}fp64_active\",\n",
    "    f\"{DCGM_PREFIX}tensor_active\",\n",
    "    f\"{DCGM_PREFIX}dram_active\",\n",
    "]\n",
    "\n",
    "# A100 peaks (FLOP/s)\n",
    "PEAK_FLOPS_FP16        = 78.0e12\n",
    "PEAK_FLOPS_FP32        = 19.5e12\n",
    "PEAK_FLOPS_FP64_VECTOR = 9.7e12\n",
    "PEAK_FLOPS_FP64_TENSOR = 19.5e12\n",
    "\n",
    "# HBM peaks (B/s)\n",
    "HBM_40 = 1.555e12\n",
    "HBM_80 = 2.039e12\n",
    "\n",
    "\n",
    "def load_nodes_from_file(path: str) -> set[str]:\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"80GB node list not found: {p.resolve()}\")\n",
    "    nodes = set()\n",
    "    for ln in p.read_text(encoding=\"utf-8\", errors=\"replace\").splitlines():\n",
    "        ln = ln.strip()\n",
    "        if not ln or ln.startswith(\"#\"):\n",
    "            continue\n",
    "        for tok in ln.replace(\",\", \" \").split():\n",
    "            tok = tok.strip()\n",
    "            if tok and not tok.startswith(\"#\"):\n",
    "                nodes.add(tok)\n",
    "    return nodes\n",
    "\n",
    "\n",
    "def read_sacct_start_end(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read sacct file and return per-JobID.\n",
    "    \"\"\"\n",
    "    p = Path(path)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"sacct file not found: {p.resolve()}\")\n",
    "\n",
    "    with p.open(\"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        header = f.readline()\n",
    "\n",
    "    if \"|\" in header:\n",
    "        names = header.rstrip(\"\\n\").split(\"|\")\n",
    "        ncols = len(names)\n",
    "        rows = []\n",
    "        with p.open(\"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "            next(f)\n",
    "            for ln in f:\n",
    "                parts = ln.rstrip(\"\\n\").split(\"|\", ncols - 1)\n",
    "                if len(parts) < ncols:\n",
    "                    parts += [\"\"] * (ncols - len(parts))\n",
    "                rows.append(parts)\n",
    "        df = pd.DataFrame(rows, columns=names)\n",
    "    else:\n",
    "        df = pd.read_csv(p, low_memory=False)\n",
    "\n",
    "    for c in [\"JobID\", \"Start\", \"End\"]:\n",
    "        if c not in df.columns:\n",
    "            raise KeyError(f\"sacct file missing required column: {c}\")\n",
    "\n",
    "    df[\"JobID\"] = df[\"JobID\"].astype(str)\n",
    "\n",
    "    start = pd.to_datetime(df[\"Start\"], errors=\"coerce\")\n",
    "    end   = pd.to_datetime(df[\"End\"],   errors=\"coerce\")\n",
    "\n",
    "    start = start.dt.tz_localize(SACCT_TZ, ambiguous=\"NaT\", nonexistent=\"NaT\").dt.tz_convert(\"UTC\")\n",
    "    end   = end.dt.tz_localize(SACCT_TZ, ambiguous=\"NaT\", nonexistent=\"NaT\").dt.tz_convert(\"UTC\")\n",
    "\n",
    "    out = pd.DataFrame({\"JobID\": df[\"JobID\"], \"start_time\": start, \"end_time\": end})\n",
    "    out = out.dropna(subset=[\"start_time\", \"end_time\"])\n",
    "\n",
    "    out = (out.groupby(\"JobID\", as_index=False)\n",
    "              .agg(start_time=(\"start_time\", \"min\"),\n",
    "                   end_time=(\"end_time\", \"max\")))\n",
    "    return out\n",
    "\n",
    "\n",
    "def build_fp64only_job_fractions_and_ngpus(\n",
    "    set1_path: str,\n",
    "    nodes80: set[str],\n",
    "    out_labels: str,\n",
    "    batch_size: int = BATCH_SET1,\n",
    "):\n",
    "    need = [\"JobID\", \"hostname\", \"gpu_id\", \"ts_ns\"] + ACTIVE_COLS\n",
    "    pf = pq.ParquetFile(set1_path)\n",
    "    present = set(pf.schema_arrow.names)\n",
    "    missing = [c for c in need if c not in present]\n",
    "    if missing:\n",
    "        raise KeyError(f\"Missing required columns in {set1_path}: {missing}\")\n",
    "\n",
    "    nodes80 = set(nodes80)\n",
    "\n",
    "    state = {} \n",
    "    job_time = defaultdict(lambda: {\"Compute-intensive\": 0, \"Memory-intensive\": 0, \"Other\": 0})\n",
    "    job_total = defaultdict(int)\n",
    "    job_samples = defaultdict(int)\n",
    "\n",
    "    rows_in = 0\n",
    "    dropped_gt1 = 0\n",
    "    dropped_all_fp_zero = 0\n",
    "\n",
    "    for batch in pf.iter_batches(columns=need, batch_size=batch_size):\n",
    "        df = batch.to_pandas()\n",
    "        if df.empty:\n",
    "            continue\n",
    "        rows_in += len(df)\n",
    "\n",
    "        job = df[\"JobID\"].astype(str)\n",
    "        host = df[\"hostname\"].astype(str)\n",
    "        gpu = pd.to_numeric(df[\"gpu_id\"], errors=\"coerce\")\n",
    "        ts  = pd.to_numeric(df[\"ts_ns\"], errors=\"coerce\")\n",
    "\n",
    "        fp16 = pd.to_numeric(df[f\"{DCGM_PREFIX}fp16_active\"], errors=\"coerce\")\n",
    "        fp32 = pd.to_numeric(df[f\"{DCGM_PREFIX}fp32_active\"], errors=\"coerce\")\n",
    "        fp64 = pd.to_numeric(df[f\"{DCGM_PREFIX}fp64_active\"], errors=\"coerce\")\n",
    "        tens = pd.to_numeric(df[f\"{DCGM_PREFIX}tensor_active\"], errors=\"coerce\")\n",
    "        dram = pd.to_numeric(df[f\"{DCGM_PREFIX}dram_active\"], errors=\"coerce\")\n",
    "\n",
    "        missing_any = fp16.isna() | fp32.isna() | fp64.isna() | tens.isna() | dram.isna()\n",
    "\n",
    "        bad_gt1 = (fp16 > 1.0) | (fp32 > 1.0) | (fp64 > 1.0) | (tens > 1.0) | (dram > 1.0)\n",
    "        dropped_gt1 += int(bad_gt1.sum())\n",
    "\n",
    "        all_fp_zero = (fp16.eq(0.0)) & (fp32.eq(0.0)) & (fp64.eq(0.0)) & (tens.eq(0.0))\n",
    "        dropped_all_fp_zero += int(all_fp_zero.sum())\n",
    "\n",
    "        keep = ~(bad_gt1 | all_fp_zero)\n",
    "        if not keep.any():\n",
    "            continue\n",
    "\n",
    "        job = job.loc[keep].to_numpy()\n",
    "        host = host.loc[keep].to_numpy()\n",
    "        gpu = gpu.loc[keep].to_numpy()\n",
    "        ts  = ts.loc[keep].to_numpy()\n",
    "\n",
    "        fp16 = fp16.loc[keep].to_numpy(dtype=float, copy=False)\n",
    "        fp32 = fp32.loc[keep].to_numpy(dtype=float, copy=False)\n",
    "        fp64 = fp64.loc[keep].to_numpy(dtype=float, copy=False)\n",
    "        tens = tens.loc[keep].to_numpy(dtype=float, copy=False)\n",
    "        dram = dram.loc[keep].to_numpy(dtype=float, copy=False)\n",
    "        missing_any = missing_any.loc[keep].to_numpy(dtype=bool, copy=False)\n",
    "\n",
    "        is80 = np.fromiter((h in nodes80 for h in host), dtype=bool, count=len(host))\n",
    "        peak_hbm = np.where(is80, HBM_80, HBM_40)\n",
    "        achieved_hbm = dram * peak_hbm\n",
    "\n",
    "        ridge_fp64 = PEAK_FLOPS_FP64_VECTOR / peak_hbm\n",
    "        achieved_fp64 = fp64 * PEAK_FLOPS_FP64_VECTOR\n",
    "\n",
    "        ai_fp64 = np.full(len(job), np.nan, dtype=float)\n",
    "        np.divide(achieved_fp64, achieved_hbm, out=ai_fp64, where=(achieved_hbm > 0))\n",
    "\n",
    "        any_compute = (fp16 > 0) | (fp32 > 0) | (fp64 > 0) | (tens > 0)\n",
    "        eligible = ~missing_any\n",
    "\n",
    "        labels = np.full(len(job), \"Other\", dtype=object)\n",
    "\n",
    "        mask_inf = eligible & any_compute & (achieved_hbm <= 0)\n",
    "        labels[mask_inf] = \"Compute-intensive\"\n",
    "\n",
    "        mask_pos = eligible & (achieved_hbm > 0) & np.isfinite(ai_fp64) & np.isfinite(ridge_fp64)\n",
    "        mem = mask_pos & (ai_fp64 < ridge_fp64)\n",
    "        comp = mask_pos & ~mem\n",
    "        labels[mem] = \"Memory-intensive\"\n",
    "        labels[comp] = \"Compute-intensive\"\n",
    "\n",
    "        for j, h, g, t, lab in zip(job, host, gpu, ts, labels):\n",
    "            job_samples[j] += 1\n",
    "\n",
    "            if not np.isfinite(g) or not np.isfinite(t):\n",
    "                continue\n",
    "            key = (j, h, int(g))\n",
    "            t = int(t)\n",
    "\n",
    "            if key in state:\n",
    "                last_ts, last_lab = state[key]\n",
    "                dt = t - last_ts\n",
    "                if dt > 0:\n",
    "                    job_time[j][last_lab] += dt\n",
    "                    job_total[j] += dt\n",
    "\n",
    "            state[key] = (t, lab)\n",
    "\n",
    "    job_ngpus = defaultdict(int)\n",
    "    for (j, _h, _g) in state.keys():\n",
    "        job_ngpus[j] += 1\n",
    "\n",
    "    rows = []\n",
    "    for j, tot in job_total.items():\n",
    "        if tot <= 0:\n",
    "            continue\n",
    "        c = job_time[j][\"Compute-intensive\"]\n",
    "        m = job_time[j][\"Memory-intensive\"]\n",
    "        o = job_time[j][\"Other\"]\n",
    "        rows.append({\n",
    "            \"JobID\": j,\n",
    "            \"time_seconds\": float(tot) / 1e9,\n",
    "            \"frac_time_compute_fp64only\": c / tot,\n",
    "            \"frac_time_memory_fp64only\":  m / tot,\n",
    "            \"frac_time_other_fp64only\":   o / tot,\n",
    "            \"sample_count\": int(job_samples.get(j, 0)),\n",
    "        })\n",
    "\n",
    "    out = pd.DataFrame(rows).sort_values(\"JobID\")\n",
    "    out.to_parquet(out_labels, index=False)\n",
    "\n",
    "    return job_ngpus\n",
    "\n",
    "\n",
    "def build_job_metrics_gpu_hours(job_ngpus: dict, sacct_csv: str, out_path: str):\n",
    "    sacct = read_sacct_start_end(sacct_csv)\n",
    "\n",
    "    sacct[\"duration_hours\"] = (sacct[\"end_time\"] - sacct[\"start_time\"]).dt.total_seconds() / 3600.0\n",
    "    sacct[\"duration_hours\"] = sacct[\"duration_hours\"].clip(lower=0)\n",
    "\n",
    "    ng = pd.Series(job_ngpus, name=\"ngpus\").astype(int)\n",
    "    ng.index = ng.index.astype(str)\n",
    "    ng = ng.reset_index().rename(columns={\"index\": \"JobID\"})\n",
    "\n",
    "    df = sacct.merge(ng, on=\"JobID\", how=\"inner\")\n",
    "    df[\"gpu_hours\"] = df[\"duration_hours\"] * df[\"ngpus\"]\n",
    "\n",
    "    out = df[[\"JobID\", \"gpu_hours\"]].copy()\n",
    "    out.to_parquet(out_path, index=False)\n",
    "\n",
    "\n",
    "def build_energy_per_job(set2_path: str, out_path: str):\n",
    "    COL_JOB = \"JobID\"\n",
    "    COL_HOST = \"hostname\"\n",
    "    COL_GPU = \"gpu_id\"\n",
    "    COL_TS = \"ts_ns\"\n",
    "    COL_ENE = \"nersc_ldms_dcgm_total_energy_consumption\"\n",
    "\n",
    "    dataset = ds.dataset(set2_path, format=\"parquet\")\n",
    "    scanner = dataset.scanner(\n",
    "        columns=[COL_JOB, COL_HOST, COL_GPU, COL_TS, COL_ENE],\n",
    "        batch_size=BATCH_SET2,\n",
    "        use_threads=True,\n",
    "    )\n",
    "\n",
    "    state = {}\n",
    "    total_rows = 0\n",
    "\n",
    "    for batch in scanner.to_batches():\n",
    "        total_rows += batch.num_rows\n",
    "        pdf = pa.Table.from_batches([batch]).to_pandas(types_mapper=pd.ArrowDtype)\n",
    "\n",
    "        pdf[COL_JOB]  = pdf[COL_JOB].astype(\"string\")\n",
    "        pdf[COL_HOST] = pdf[COL_HOST].astype(\"string\")\n",
    "        pdf[COL_GPU]  = pd.to_numeric(pdf[COL_GPU], errors=\"coerce\").astype(\"Int64\")\n",
    "        pdf[COL_TS]   = pd.to_numeric(pdf[COL_TS],  errors=\"coerce\").astype(\"Int64\")\n",
    "        pdf[COL_ENE]  = pd.to_numeric(pdf[COL_ENE], errors=\"coerce\").astype(\"Float64\")\n",
    "\n",
    "        pdf = pdf.dropna(subset=[COL_TS, COL_ENE, COL_GPU, COL_JOB, COL_HOST])\n",
    "        if pdf.empty:\n",
    "            continue\n",
    "\n",
    "        grp = pdf.groupby([COL_JOB, COL_HOST, COL_GPU], sort=False, observed=True)\n",
    "        counts = grp.size().reset_index(name=\"n\").set_index([COL_JOB, COL_HOST, COL_GPU])\n",
    "\n",
    "        first_rows = (\n",
    "            pdf.drop_duplicates(subset=[COL_JOB, COL_HOST, COL_GPU], keep=\"first\")\n",
    "               .set_index([COL_JOB, COL_HOST, COL_GPU])[[COL_ENE]]\n",
    "               .rename(columns={COL_ENE: \"first_e\"})\n",
    "        )\n",
    "        last_rows = (\n",
    "            pdf.drop_duplicates(subset=[COL_JOB, COL_HOST, COL_GPU], keep=\"last\")\n",
    "               .set_index([COL_JOB, COL_HOST, COL_GPU])[[COL_ENE]]\n",
    "               .rename(columns={COL_ENE: \"last_e\"})\n",
    "        )\n",
    "\n",
    "        batch_keys = set((str(i[0]), str(i[1]), int(i[2])) for i in counts.index)\n",
    "\n",
    "        for (j, h, g) in batch_keys:\n",
    "            n_in_batch = int(counts.loc[(j, h, g), \"n\"])\n",
    "            fe = float(first_rows.loc[(j, h, g), \"first_e\"])\n",
    "            le = float(last_rows.loc[(j, h, g), \"last_e\"])\n",
    "\n",
    "            key = (j, h, g)\n",
    "            if key not in state:\n",
    "                state[key] = {\"first_e\": fe, \"last_e\": le, \"n\": n_in_batch}\n",
    "            else:\n",
    "                state[key][\"last_e\"] = le\n",
    "                state[key][\"n\"] += n_in_batch\n",
    "\n",
    "    rows_gpu = []\n",
    "    for (j, h, g), st in state.items():\n",
    "        rows_gpu.append((j, st[\"last_e\"] - st[\"first_e\"]))\n",
    "\n",
    "    df_gpu = pd.DataFrame(rows_gpu, columns=[\"JobID\", \"energy_delta\"])\n",
    "    df_job = df_gpu.groupby(\"JobID\", as_index=False).agg(total_energy_joules=(\"energy_delta\", \"sum\"))\n",
    "\n",
    "    df_job.to_parquet(out_path, index=False)\n",
    "\n",
    "\n",
    "def plot_energy_vs_gpu_hours():\n",
    "    LABELS_FILE_LOCAL = LABELS_FILE\n",
    "    ENERGY_PER_JOB_LOCAL = ENERGY_PER_JOB\n",
    "    JOB_METRICS_FILE_LOCAL = JOB_METRICS_FILE\n",
    "\n",
    "    lab = pd.read_parquet(\n",
    "        LABELS_FILE_LOCAL,\n",
    "        columns=[\"JobID\",\"frac_time_compute_fp64only\",\"frac_time_memory_fp64only\"]\n",
    "    ).astype({\"JobID\": str})\n",
    "\n",
    "    comp_mask = lab[\"frac_time_compute_fp64only\"] > lab[\"frac_time_memory_fp64only\"]\n",
    "    mem_mask  = lab[\"frac_time_memory_fp64only\"]  > lab[\"frac_time_compute_fp64only\"]\n",
    "\n",
    "    labels = pd.DataFrame({\n",
    "        \"JobID\": lab.loc[comp_mask | mem_mask, \"JobID\"],\n",
    "        \"class\": np.where(comp_mask[comp_mask | mem_mask], \"Compute-bound\", \"Memory-bound\")\n",
    "    })\n",
    "\n",
    "    epj = (pd.read_parquet(ENERGY_PER_JOB_LOCAL, columns=[\"JobID\",\"total_energy_joules\"])\n",
    "             .astype({\"JobID\": str})\n",
    "             .rename(columns={\"total_energy_joules\":\"total_energy_mj\"}))\n",
    "    epj[\"total_energy_j\"] = pd.to_numeric(epj[\"total_energy_mj\"], errors=\"coerce\") / 1000.0\n",
    "    epj = epj[[\"JobID\",\"total_energy_j\"]]\n",
    "\n",
    "    jm = (pd.read_parquet(JOB_METRICS_FILE_LOCAL, columns=[\"JobID\",\"gpu_hours\"])\n",
    "            .astype({\"JobID\": str}))\n",
    "    jm[\"gpu_hours\"] = pd.to_numeric(jm[\"gpu_hours\"], errors=\"coerce\")\n",
    "\n",
    "    df = (labels.merge(epj, on=\"JobID\", how=\"inner\")\n",
    "                .merge(jm,  on=\"JobID\", how=\"inner\"))\n",
    "\n",
    "\n",
    "    df = df[(df[\"total_energy_j\"] > 0) & (df[\"gpu_hours\"] >= 0.5)].copy()\n",
    "\n",
    "    major_ticks = np.array([1e-1, 1e0, 1e1, 1e2, 1e3, 1e4, 1e5], dtype=float)\n",
    "    edges_list = []\n",
    "    for i in range(len(major_ticks)-1):\n",
    "        a, b = major_ticks[i], major_ticks[i+1]\n",
    "        sub_edges = a * (b/a)**(np.linspace(0, 1, 3))\n",
    "        if i > 0:\n",
    "            sub_edges = sub_edges[1:]\n",
    "        edges_list.append(sub_edges)\n",
    "    edges = np.concatenate(edges_list) \n",
    "    widths = edges[1:] - edges[:-1]\n",
    "\n",
    "    bin_idx = np.digitize(df[\"gpu_hours\"].to_numpy(dtype=float), edges, right=False) - 1\n",
    "    bin_idx = np.where(bin_idx >= len(edges)-1, len(edges)-2, bin_idx)\n",
    "    bin_idx = np.where(bin_idx < 0, 0, bin_idx)\n",
    "    df[\"gh_bin\"] = bin_idx\n",
    "\n",
    "    def means_per_bin(sub_df: pd.DataFrame, nbins: int):\n",
    "        grp = sub_df.groupby(\"gh_bin\")[\"total_energy_j\"]\n",
    "        sum_by_bin   = grp.sum()\n",
    "        count_by_bin = grp.count()\n",
    "        sums   = np.array([sum_by_bin.get(i, 0.0) for i in range(nbins)], dtype=float)\n",
    "        counts = np.array([count_by_bin.get(i, 0) for i in range(nbins)], dtype=float)\n",
    "        means  = np.divide(sums, counts, out=np.zeros_like(sums), where=counts>0)\n",
    "        return means, counts\n",
    "\n",
    "    nbins = len(edges) - 1\n",
    "    means_comp, counts_comp = means_per_bin(df[df[\"class\"] == \"Compute-bound\"], nbins)\n",
    "    means_mem,  counts_mem  = means_per_bin(df[df[\"class\"] == \"Memory-bound\"],  nbins)\n",
    "\n",
    "    setup_local()\n",
    "    colors  = get_colors()\n",
    "    markers = get_markers()\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    def values_per_bin(sub_df: pd.DataFrame, nbins: int):\n",
    "        return [sub_df.loc[sub_df[\"gh_bin\"] == i, \"total_energy_j\"].to_numpy() for i in range(nbins)]\n",
    "\n",
    "    vals_comp = values_per_bin(df[df[\"class\"] == \"Compute-bound\"], nbins)\n",
    "    vals_mem  = values_per_bin(df[df[\"class\"] == \"Memory-bound\"],  nbins)\n",
    "\n",
    "    centers = np.sqrt(edges[:-1] * edges[1:])\n",
    "    offset  = widths * 0.2\n",
    "    pos_comp = centers - offset\n",
    "    pos_mem  = centers + offset\n",
    "\n",
    "    box_widths_comp = (widths * 0.40) * (pos_comp / centers)\n",
    "    box_widths_mem  = (widths * 0.40) * (pos_mem  / centers)\n",
    "\n",
    "    bp_comp = ax.boxplot(\n",
    "        vals_comp, positions=pos_comp, widths=box_widths_comp,\n",
    "        manage_ticks=False, patch_artist=True, showfliers=False,\n",
    "        whis=[5, 95], showmeans=False,\n",
    "        meanprops=dict(marker=markers[1], markerfacecolor=colors[5], markeredgecolor=colors[5]),\n",
    "    )\n",
    "    bp_mem = ax.boxplot(\n",
    "        vals_mem, positions=pos_mem, widths=box_widths_mem,\n",
    "        manage_ticks=False, patch_artist=True, showfliers=False,\n",
    "        whis=[5, 95], showmeans=False,\n",
    "        meanprops=dict(marker=markers[1], markerfacecolor=colors[5], markeredgecolor=colors[5]),\n",
    "    )\n",
    "\n",
    "    for elem in [\"boxes\",\"whiskers\",\"caps\",\"medians\"]:\n",
    "        for artist in bp_comp[elem]:\n",
    "            artist.set_color(\"black\")\n",
    "            if elem == \"medians\":\n",
    "                artist.set_color(\"white\")\n",
    "    for patch in bp_comp[\"boxes\"]:\n",
    "        patch.set_facecolor(colors[4]); patch.set_alpha(1); patch.set_edgecolor(\"black\")\n",
    "\n",
    "    for elem in [\"boxes\",\"whiskers\",\"caps\",\"medians\"]:\n",
    "        for artist in bp_mem[elem]:\n",
    "            artist.set_color(\"black\")\n",
    "            if elem == \"medians\": \n",
    "                artist.set_color(\"white\")\n",
    "    for patch in bp_mem[\"boxes\"]:\n",
    "        patch.set_facecolor(colors[5]); patch.set_alpha(1); patch.set_edgecolor(\"black\")\n",
    "\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.set_xlim(1e-1, 1e5)\n",
    "    ax.set_ylim(1e5, 1e11)  \n",
    "\n",
    "    major_ticks_list = [1e-1, 1e0, 1e1, 1e2, 1e3, 1e4, 1e5]\n",
    "    ax.xaxis.set_major_locator(FixedLocator(major_ticks_list))\n",
    "    ax.xaxis.set_major_formatter(LogFormatterMathtext(base=10))\n",
    "    ax.xaxis.set_minor_locator(NullLocator())\n",
    "\n",
    "    ax.set_yticks([1e5, 1e7, 1e9, 1e11])\n",
    "\n",
    "    ax.set_xlabel(\"GPU-hours (number of GPUs x duration)\", fontsize=17)\n",
    "    ax.set_ylabel(\"Total energy per job (J)\", fontsize=17)\n",
    "    ax.set_title(\"Distribution of total energy consumption\", pad=14, fontsize=18)\n",
    "    ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "    ax.tick_params(axis=\"x\", labelsize=16)\n",
    "    ax.tick_params(axis=\"y\", labelsize=16)\n",
    "\n",
    "    ax.legend(\n",
    "        frameon=True, fontsize=14, loc=\"upper left\",\n",
    "        handles=[\n",
    "            plt.Rectangle((0,0),1,1,facecolor=colors[4], alpha=1, edgecolor=\"black\", label=\"Compute-bound\"),\n",
    "            plt.Rectangle((0,0),1,1,facecolor=colors[5], alpha=1, edgecolor=\"black\", label=\"Memory-bound\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    fig.set_size_inches(7, 3, forward=True)\n",
    "    fig.subplots_adjust(left=0.12, right=0.98, bottom=0.19, top=0.86)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    nodes_80gb_set = load_nodes_from_file(NODES_80GB_FILE)\n",
    "\n",
    "    job_ngpus = build_fp64only_job_fractions_and_ngpus(SET1_PARQUET, nodes_80gb_set, LABELS_FILE)\n",
    "\n",
    "    build_job_metrics_gpu_hours(job_ngpus, SACCT_CSV, JOB_METRICS_FILE)\n",
    "\n",
    "    build_energy_per_job(SET2_PARQUET, ENERGY_PER_JOB)\n",
    "\n",
    "    plot_energy_vs_gpu_hours()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
